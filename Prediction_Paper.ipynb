{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSM8Ky3xfGcv",
        "outputId": "abcaf57f-083b-4de8-bc32-170e1eac5f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all seasons befor dropna: (1140, 25)\n",
            "Data loaded and processed. Total shape: (1140, 26)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# --- 1. LOAD DATA (CONSOLIDATED) ---\n",
        "\n",
        "# Paths to your files\n",
        "path_prefix = '/content/drive/MyDrive/archive-2/Data_with_90_minutes/'\n",
        "file_paths = {\n",
        "    '19-20': '2019-20_weather_90_full.csv',\n",
        "    '20-21': '2020-21_weather_90.csv',\n",
        "    '21-22': '2021-22_weather_90.csv',\n",
        "}\n",
        "\n",
        "# Columns we need\n",
        "columns_req = [\n",
        "    'Date', 'Time',\n",
        "    'HomeTeam', 'AwayTeam', 'FTR',\n",
        "    'FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST',\n",
        "    'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR',\n",
        "    'Start_Temp_C', 'Start_Wind_kmh', 'Start_Wind_Degree',\n",
        "    'Start_Humidity', 'Start_Precip_mm', 'Start_Conditions'\n",
        "]\n",
        "\n",
        "data_frames = []\n",
        "\n",
        "# Load and prepare all CSVs\n",
        "for key, file_path in file_paths.items():\n",
        "    try:\n",
        "        df = pd.read_csv(f\"{path_prefix}{file_path}\")\n",
        "        df = df[columns_req]\n",
        "\n",
        "        # Correct/unify date format\n",
        "        df.loc[:,'Date'] = pd.to_datetime(df['Date'], #format='%d/%m/%y',\n",
        "                                          errors='coerce', dayfirst=True)\n",
        "\n",
        "        data_frames.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "# --- 2. CREATE SINGLE MASTER DATAFRAME ---\n",
        "\n",
        "all_seasons_df = pd.concat(data_frames, ignore_index=True)\n",
        "print(f'all seasons befor dropna: {all_seasons_df.shape}')\n",
        "all_seasons_df = all_seasons_df.dropna(subset=['Date']) # Remove rows with invalid dates\n",
        "all_seasons_df = all_seasons_df.sort_values(by='Date').reset_index(drop=True)\n",
        "all_seasons_df['match_id'] = all_seasons_df.index\n",
        "\n",
        "print(f\"Data loaded and processed. Total shape: {all_seasons_df.shape}\")\n",
        "all_seasons_df.head(10)\n",
        "all_seasons_df.to_csv(\"all_seasons.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding für Wetterdaten"
      ],
      "metadata": {
        "id": "fJD6PoDAUN9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_work = all_seasons_df\n",
        "\n",
        "print(\"Starting weather-classification for DataFrame...\")\n",
        "\n",
        "def map_weather_description(description):\n",
        "    \"\"\"\n",
        "    Sorts weather descriptions into a main category based on severity.\n",
        "    Order: Dangerous -> Precipitation -> Visibility -> Cloud cover -> Clear\n",
        "    \"\"\"\n",
        "    if pd.isna(description):\n",
        "        return 'Unknown'\n",
        "\n",
        "    desc = str(description).lower()\n",
        "\n",
        "    # 1. Thunderstorm\n",
        "    if 'thunderstorm' in desc:\n",
        "        return 'thunderstorm'\n",
        "\n",
        "    # 2. snow and ice\n",
        "    if 'snow' in desc or 'sleet' in desc or 'freezing' in desc:\n",
        "        return 'snow'\n",
        "\n",
        "    # 3. heavy rain\n",
        "    if 'heavy' in desc or 'extreme' in desc or 'ragged' in desc:\n",
        "        return 'heavy_rain'\n",
        "\n",
        "    # 4. moderate rain\n",
        "    if 'moderate' in desc or 'shower' in desc:\n",
        "        return 'moderate_rain'\n",
        "\n",
        "    # 5. Rain\n",
        "    if 'rain' in desc or 'drizzle' in desc:\n",
        "        return 'rain'\n",
        "\n",
        "    # 6. Visibility\n",
        "    if 'mist' in desc or 'fog' in desc or 'haze' in desc:\n",
        "        return 'fog'\n",
        "\n",
        "    # 7. Cloudy\n",
        "    if 'clouds' in desc:\n",
        "        return 'cloudy'\n",
        "\n",
        "    # 8. clear sky\n",
        "    if 'clear' in desc or 'sun' in desc:\n",
        "        return 'clear_sky'\n",
        "\n",
        "    # Fallback\n",
        "    return 'else'\n",
        "\n",
        "column_name = 'Start_Conditions'\n",
        "\n",
        "if column_name in df_work.columns:\n",
        "    print(f\"Column '{column_name}' found. Start categorization\")\n",
        "\n",
        "    # A) Categories\n",
        "    df_work['Weather_categories'] = df_work[column_name].apply(map_weather_description)\n",
        "\n",
        "    print(\"Distribution of weather categories:\")\n",
        "    print(df_work['Weather_categories'].value_counts())\n",
        "\n",
        "    # B) One-Hot Encoding\n",
        "    print(\"Apply One-Hot Encoding ...\")\n",
        "    weather_features = pd.get_dummies(df_work['Weather_categories'], prefix='Weather', dtype=int)\n",
        "\n",
        "    # C) Concate\n",
        "    df_work = pd.concat([df_work, weather_features], axis=1)\n",
        "\n",
        "    # D) clean up\n",
        "    df_work.drop([column_name, 'Weather_categories'], axis=1, inplace=True)\n",
        "\n",
        "    print(\"New Column:\", weather_features.columns.tolist())\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"Warning: column '{column_name}' not found!\")\n",
        "    print(\"Verfügbare Spalten:\", df_work.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxqmPAAnYsb4",
        "outputId": "51eb9010-bbfd-4cee-865b-c2742e59f827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting weather-classification for DataFrame...\n",
            "Column 'Start_Conditions' found. Start categorization\n",
            "Distribution of weather categories:\n",
            "Weather_categories\n",
            "cloudy           585\n",
            "rain             234\n",
            "clear_sky        208\n",
            "moderate_rain     69\n",
            "fog               34\n",
            "snow               6\n",
            "heavy_rain         3\n",
            "thunderstorm       1\n",
            "Name: count, dtype: int64\n",
            "Apply One-Hot Encoding ...\n",
            "New Column: ['Weather_clear_sky', 'Weather_cloudy', 'Weather_fog', 'Weather_heavy_rain', 'Weather_moderate_rain', 'Weather_rain', 'Weather_snow', 'Weather_thunderstorm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature-Functions"
      ],
      "metadata": {
        "id": "yt5Toyq3fyNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_kickoff_time(df):\n",
        "    \"\"\"\n",
        "    Classifies the kickoff time ('Time') into Afternoon (0) or Evening (1).\n",
        "    Creates the 'TMKO' column.\n",
        "\n",
        "    Logic:\n",
        "    - 0 (Afternoon): Kickoff before 5 PM (17:00)\n",
        "    - 1 (Evening): Kickoff at 5 PM (17:00) or later\n",
        "    \"\"\"\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # --- STEP 0: Clean columns to prevent _x/_y suffixes ---\n",
        "    # Find all columns starting with TMKO (TMKO, TMKO_x, TMKO_y)\n",
        "    cols_to_drop = [col for col in df_out.columns if col.startswith('TMKO')]\n",
        "    if cols_to_drop:\n",
        "        df_out = df_out.drop(columns=cols_to_drop)\n",
        "\n",
        "    # --- STEP 1: Extract hour ---\n",
        "    # Ensures the 'Time' column is treated as datetime\n",
        "    # and extracts the hour as a number (e.g., \"16:30\" -> 16)\n",
        "    try:\n",
        "        # Fill missing times (NaN) with '00:00' before conversion\n",
        "        hour = pd.to_datetime(df_out['Time'].fillna('00:00'), format='%H:%M').dt.hour\n",
        "    except ValueError as e:\n",
        "        print(f\"Error converting the 'Time' column: {e}\")\n",
        "        print(\"Ensure all times are in 'HH:MM' format (e.g., 16:30).\")\n",
        "        # Show a few problematic values\n",
        "        print(\"Problem data (examples):\")\n",
        "        print(df_out[pd.to_datetime(df_out['Time'], format='%H:%M', errors='coerce').isna()]['Time'].head())\n",
        "        return df # Return original DF if an error occurs\n",
        "\n",
        "    # --- STEP 2: Classify ---\n",
        "    # np.where(condition, value_if_True, value_if_False)\n",
        "    df_out['TMKO'] = np.where(hour >= 17, 1, 0)\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def get_strict_rolling_avg(df, stats_map, window_size=3):\n",
        "    \"\"\"\n",
        "    Calculates the rolling average of the LAST 'window_size' matches.\n",
        "    The CURRENT match is NOT included in the calculation.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Create a \"long\" version of the data\n",
        "    generic_cols = list(stats_map.keys())\n",
        "    home_df = df[['match_id', 'Date', 'HomeTeam'] + generic_cols].copy()\n",
        "    home_df.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "    home_df['is_home_match'] = 1\n",
        "\n",
        "    away_cols = list(stats_map.values())\n",
        "    away_df = df[['match_id', 'Date', 'AwayTeam'] + away_cols].copy()\n",
        "    away_df.rename(columns={'AwayTeam': 'Team'}, inplace=True)\n",
        "    away_rename_map = dict(zip(away_cols, generic_cols))\n",
        "    away_df.rename(columns=away_rename_map, inplace=True)\n",
        "    away_df['is_home_match'] = 0\n",
        "\n",
        "    # 2. Combine and sort by date\n",
        "    all_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
        "    all_stats_df = all_stats_df.sort_values(by=['Team', 'Date'])\n",
        "\n",
        "    # 3. Calculate and assign rolling average\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Prepare temporary DataFrames for home/away merges\n",
        "    home_merge_df = all_stats_df[all_stats_df['is_home_match'] == 1][['match_id', 'Team']].copy()\n",
        "    home_merge_df.rename(columns={'Team': 'HomeTeam'}, inplace=True)\n",
        "    away_merge_df = all_stats_df[all_stats_df['is_home_match'] == 0][['match_id', 'Team']].copy()\n",
        "    away_merge_df.rename(columns={'Team': 'AwayTeam'}, inplace=True)\n",
        "\n",
        "    for home_stat, away_stat in stats_map.items():\n",
        "        # 1. Calculate rolling avg (includes current match)\n",
        "        rolling_avg_series = all_stats_df.groupby('Team')[home_stat].rolling(window=window_size).mean()\n",
        "        rolling_avg_series = rolling_avg_series.reset_index(level=0, drop=True)\n",
        "\n",
        "        # 2. Assign average as a *temporary* column\n",
        "        all_stats_df['TEMP_AVG'] = rolling_avg_series\n",
        "\n",
        "        # 3. Get the average value from the PREVIOUS row (.shift(1))\n",
        "        all_stats_df['PREV_AVG'] = all_stats_df.groupby('Team')['TEMP_AVG'].shift(1)\n",
        "\n",
        "        # --- 4. Merge back to \"wide\" format ---\n",
        "\n",
        "        # Get home team data and rename column (e.g., 'AHST')\n",
        "        home_data = all_stats_df[all_stats_df['is_home_match'] == 1][['match_id', 'PREV_AVG']]\n",
        "        home_data = home_data.rename(columns={'PREV_AVG': f'A{home_stat}'})\n",
        "        home_merge_df = home_merge_df.merge(home_data, on='match_id', how='left')\n",
        "\n",
        "        # Get away team data and rename column (e.g., 'AAST')\n",
        "        away_data = all_stats_df[all_stats_df['is_home_match'] == 0][['match_id', 'PREV_AVG']]\n",
        "        away_data = away_data.rename(columns={'PREV_AVG': f'A{away_stat}'})\n",
        "        away_merge_df = away_merge_df.merge(away_data, on='match_id', how='left')\n",
        "\n",
        "    # --- 5. Final Merges ---\n",
        "    df_out = df_out.merge(home_merge_df, on=['match_id', 'HomeTeam'], how='left')\n",
        "    df_out = df_out.merge(away_merge_df, on=['match_id', 'AwayTeam'], how='left')\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def get_previous_game_form(df):\n",
        "    \"\"\"\n",
        "    Determines the result (Won/NotWin) of each team's last match.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate result (Form) for each match from the team's perspective\n",
        "\n",
        "    # --- Home Data ---\n",
        "    home_df = df[['match_id', 'Date', 'HomeTeam', 'FTR']].copy()\n",
        "    home_df.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "    home_df['is_home_match'] = 1\n",
        "    home_df['Won'] = (home_df['FTR'] == 'H').astype(int)\n",
        "    home_df['NotWin'] = (home_df['FTR'] != 'H').astype(int)\n",
        "\n",
        "    # --- Away Data ---\n",
        "    away_df = df[['match_id', 'Date', 'AwayTeam', 'FTR']].copy()\n",
        "    away_df.rename(columns={'AwayTeam': 'Team'}, inplace=True)\n",
        "    away_df['is_home_match'] = 0\n",
        "    away_df['Won'] = (away_df['FTR'] == 'A').astype(int)\n",
        "    away_df['NotWin'] = (away_df['FTR'] != 'A').astype(int)\n",
        "\n",
        "    # 2. Combine and sort by date\n",
        "    all_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
        "    all_stats_df = all_stats_df.sort_values(by=['Team', 'Date'])\n",
        "\n",
        "    # 3. Create \"Previous\" form for each team\n",
        "    all_stats_df['P_Won'] = all_stats_df.groupby('Team')['Won'].shift(1)\n",
        "    all_stats_df['P_NotWin'] = all_stats_df.groupby('Team')['NotWin'].shift(1)\n",
        "\n",
        "    # 4. Merge back to \"wide\" format\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Home teams\n",
        "    home_prev_data = all_stats_df[all_stats_df['is_home_match'] == 1].copy()\n",
        "    home_prev_data.rename(columns={'Team': 'HomeTeam', 'P_Won': 'PHFR_Won', 'P_NotWin': 'PHFR_NotWin'}, inplace=True)\n",
        "    cols_to_merge_home = ['match_id', 'HomeTeam', 'PHFR_Won', 'PHFR_NotWin']\n",
        "    df_out = df_out.merge(home_prev_data[cols_to_merge_home], on=['match_id', 'HomeTeam'], how='left')\n",
        "\n",
        "    # Away teams\n",
        "    away_prev_data = all_stats_df[all_stats_df['is_home_match'] == 0].copy()\n",
        "    away_prev_data.rename(columns={'Team': 'AwayTeam', 'P_Won': 'PAFR_Won', 'P_NotWin': 'PAFR_NotWin'}, inplace=True)\n",
        "    cols_to_merge_away = ['match_id', 'AwayTeam', 'PAFR_Won', 'PAFR_NotWin']\n",
        "    df_out = df_out.merge(away_prev_data[cols_to_merge_away], on=['match_id', 'AwayTeam'], how='left')\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def get_last_match_overall(df, stats_map):\n",
        "    \"\"\"\n",
        "    Gets the stats of the last match for each team,\n",
        "    regardless of venue (Home or Away).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Create a \"long\" version of the data\n",
        "    generic_cols = list(stats_map.keys())\n",
        "    home_df = df[['match_id', 'Date', 'HomeTeam'] + generic_cols].copy()\n",
        "    home_df.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "    home_df['is_home_match'] = 1\n",
        "\n",
        "    away_cols = list(stats_map.values())\n",
        "    away_df = df[['match_id', 'Date', 'AwayTeam'] + away_cols].copy()\n",
        "    away_df.rename(columns={'AwayTeam': 'Team'}, inplace=True)\n",
        "    away_rename_map = dict(zip(away_cols, generic_cols))\n",
        "    away_df.rename(columns=away_rename_map, inplace=True)\n",
        "    away_df['is_home_match'] = 0\n",
        "\n",
        "    # 2. Combine and sort by date\n",
        "    all_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
        "    all_stats_df = all_stats_df.sort_values(by=['Team', 'Date'])\n",
        "\n",
        "    # 3. Create \"Previous\" stats for each team\n",
        "    prev_cols = {}\n",
        "    for col in generic_cols:\n",
        "        prev_cols[f'P_{col}'] = all_stats_df.groupby('Team')[col].shift(1)\n",
        "\n",
        "    prev_stats_df = pd.DataFrame(prev_cols)\n",
        "    all_stats_df = pd.concat([all_stats_df, prev_stats_df], axis=1)\n",
        "\n",
        "    # 4. Merge back to \"wide\" format\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # --- Merge for Home teams ---\n",
        "    home_prev_data = all_stats_df[all_stats_df['is_home_match'] == 1].copy()\n",
        "    home_prev_data.rename(columns={'Team': 'HomeTeam'}, inplace=True)\n",
        "    # Rename columns: 'P_HST' -> 'PHST'\n",
        "    home_rename_map = {f'P_{col}': f'P{col}' for col in generic_cols}\n",
        "    home_prev_data.rename(columns=home_rename_map, inplace=True)\n",
        "    cols_to_merge_home = ['match_id', 'HomeTeam'] + list(home_rename_map.values())\n",
        "    df_out = df_out.merge(home_prev_data[cols_to_merge_home], on=['match_id', 'HomeTeam'], how='left')\n",
        "\n",
        "    # --- Merge for Away teams ---\n",
        "    away_prev_data = all_stats_df[all_stats_df['is_home_match'] == 0].copy()\n",
        "    away_prev_data.rename(columns={'Team': 'AwayTeam'}, inplace=True)\n",
        "    # Rename columns: 'P_HST' -> 'PAST' (uses the 'away_stat' name)\n",
        "    away_rename_map = {}\n",
        "    for home_stat, away_stat in stats_map.items():\n",
        "        away_rename_map[f'P_{home_stat}'] = f'P{away_stat}' # e.g., 'P_HST' -> 'PAST'\n",
        "    away_prev_data.rename(columns=away_rename_map, inplace=True)\n",
        "    cols_to_merge_away = ['match_id', 'AwayTeam'] + list(away_rename_map.values())\n",
        "    df_out = df_out.merge(away_prev_data[cols_to_merge_away], on=['match_id', 'AwayTeam'], how='left')\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "\n",
        "def get_time_between_games(df):\n",
        "    \"\"\"\n",
        "    Calculates the number of days since the last match for\n",
        "    the home and away teams (TBGH, TBGA).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # --- SCHRITT 0: cleaning columns ---\n",
        "    base_cols = ['TBGH', 'TBGA']\n",
        "\n",
        "    cols_to_drop = []\n",
        "    for col_in_df in df_out.columns:\n",
        "        base_name = col_in_df.split('_x')[0].split('_y')[0]\n",
        "        if base_name in base_cols:\n",
        "            cols_to_drop.append(col_in_df)\n",
        "\n",
        "    if cols_to_drop:\n",
        "        df_out = df_out.drop(columns=list(set(cols_to_drop)))\n",
        "\n",
        "\n",
        "    # 1.\n",
        "\n",
        "    # --- Home-Team ---\n",
        "    home_df = df_out[['match_id', 'Date', 'HomeTeam']].copy()\n",
        "    home_df.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "    home_df['is_home_match'] = 1\n",
        "\n",
        "    # --- Away-Team ---\n",
        "    away_df = df_out[['match_id', 'Date', 'AwayTeam']].copy()\n",
        "    away_df.rename(columns={'AwayTeam': 'Team'}, inplace=True)\n",
        "    away_df['is_home_match'] = 0\n",
        "\n",
        "    # 2. Concating and sorting\n",
        "    all_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
        "    all_stats_df = all_stats_df.sort_values(by=['Team', 'Date'])\n",
        "\n",
        "    # 3. \"Previous Date\": calculate differences\n",
        "\n",
        "    all_stats_df['Prev_Date'] = all_stats_df.groupby('Team')['Date'].shift(1)\n",
        "\n",
        "    # Ensureing both columns are datetime objects before subtraction\n",
        "    all_stats_df['Date'] = pd.to_datetime(all_stats_df['Date'], errors='coerce')\n",
        "    all_stats_df['Prev_Date'] = pd.to_datetime(all_stats_df['Prev_Date'], errors='coerce')\n",
        "\n",
        "    all_stats_df['TBG'] = (all_stats_df['Date'] - all_stats_df['Prev_Date']).dt.days\n",
        "\n",
        "    all_stats_df = all_stats_df.sort_values(by='Date')\n",
        "\n",
        "    # 4. Merging to DataFrame\n",
        "\n",
        "    home_prev_data = all_stats_df[all_stats_df['is_home_match'] == 1].copy()\n",
        "    home_prev_data.rename(columns={'Team': 'HomeTeam', 'TBG': 'TBGH'}, inplace=True)\n",
        "\n",
        "    cols_to_merge_home = ['match_id', 'HomeTeam', 'TBGH']\n",
        "    df_out = df_out.merge(\n",
        "        home_prev_data[cols_to_merge_home],\n",
        "        on=['match_id', 'HomeTeam'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    away_prev_data = all_stats_df[all_stats_df['is_home_match'] == 0].copy()\n",
        "    away_prev_data.rename(columns={'Team': 'AwayTeam', 'TBG': 'TBGA'}, inplace=True)\n",
        "\n",
        "    cols_to_merge_away = ['match_id', 'AwayTeam', 'TBGA']\n",
        "    df_out = df_out.merge(\n",
        "        away_prev_data[cols_to_merge_away],\n",
        "        on=['match_id', 'AwayTeam'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def get_number_of_wins(df, window_size=3):\n",
        "    \"\"\"\n",
        "    Calculates the *number* of wins in the last 'window_size' games\n",
        "    (NOWH, NOWA).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # --- Cleaning ---\n",
        "    base_cols = ['NOWH', 'NOWA']\n",
        "\n",
        "    cols_to_drop = []\n",
        "    for col_in_df in df_out.columns:\n",
        "        base_name = col_in_df.split('_x')[0].split('_y')[0]\n",
        "        if base_name in base_cols:\n",
        "            cols_to_drop.append(col_in_df)\n",
        "\n",
        "    if cols_to_drop:\n",
        "        df_out = df_out.drop(columns=list(set(cols_to_drop)))\n",
        "    # --- END cleaning ---\n",
        "\n",
        "\n",
        "    # 1.\n",
        "\n",
        "    # --- Home-Team ---\n",
        "    home_df = df_out[['match_id', 'Date', 'HomeTeam', 'FTR']].copy()\n",
        "    home_df.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "    home_df['is_home_match'] = 1\n",
        "    home_df['Won'] = (home_df['FTR'] == 'H').astype(int) # 1 für Heimsieg, 0 sonst\n",
        "\n",
        "    # --- Away-Team ---\n",
        "    away_df = df_out[['match_id', 'Date', 'AwayTeam', 'FTR']].copy()\n",
        "    away_df.rename(columns={'AwayTeam': 'Team'}, inplace=True)\n",
        "    away_df['is_home_match'] = 0\n",
        "    away_df['Won'] = (away_df['FTR'] == 'A').astype(int) # 1 für Auswärtssieg, 0 sonst\n",
        "\n",
        "    # 2. sort\n",
        "    all_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
        "    all_stats_df = all_stats_df.sort_values(by=['Team', 'Date'])\n",
        "\n",
        "    # 3. calculate sum of wins\n",
        "\n",
        "    # a. Rolling sum\n",
        "    rolling_sum_series = all_stats_df.groupby('Team')['Won'] \\\n",
        "                                     .rolling(window=window_size) \\\n",
        "                                     .sum()\n",
        "    rolling_sum_series = rolling_sum_series.reset_index(level=0, drop=True)\n",
        "\n",
        "    all_stats_df['TEMP_NOW'] = rolling_sum_series\n",
        "\n",
        "    all_stats_df['NOW'] = all_stats_df.groupby('Team')['TEMP_NOW'].shift(1)\n",
        "\n",
        "    # 4. sorting back\n",
        "    all_stats_df = all_stats_df.sort_values(by='Date')\n",
        "\n",
        "    # 5. Merging back\n",
        "\n",
        "    # --- Merge ---\n",
        "    home_prev_data = all_stats_df[all_stats_df['is_home_match'] == 1].copy()\n",
        "    home_prev_data.rename(columns={'Team': 'HomeTeam', 'NOW': 'NOWH'}, inplace=True)\n",
        "\n",
        "    cols_to_merge_home = ['match_id', 'HomeTeam', 'NOWH']\n",
        "    df_out = df_out.merge(\n",
        "        home_prev_data[cols_to_merge_home],\n",
        "        on=['match_id', 'HomeTeam'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    away_prev_data = all_stats_df[all_stats_df['is_home_match'] == 0].copy()\n",
        "    away_prev_data.rename(columns={'Team': 'AwayTeam', 'NOW': 'NOWA'}, inplace=True)\n",
        "\n",
        "    cols_to_merge_away = ['match_id', 'AwayTeam', 'NOWA']\n",
        "    df_out = df_out.merge(\n",
        "        away_prev_data[cols_to_merge_away],\n",
        "        on=['match_id', 'AwayTeam'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # clean up\n",
        "    df_out = df_out.drop(columns=['NOWH_x', 'NOWA_x', 'NOWH_y', 'NOWA_y'], errors='ignore')\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def get_avg_time_between_games(df, window_size=3):\n",
        "    \"\"\"\n",
        "    calculates the *average* number of days since the\n",
        "    last 'window_size' games (ATBGH, ATBGA).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    base_cols = ['ATBGH', 'ATBGA']\n",
        "\n",
        "    cols_to_drop = []\n",
        "    for col_in_df in df_out.columns:\n",
        "        base_name = col_in_df.split('_x')[0].split('_y')[0]\n",
        "        if base_name in base_cols:\n",
        "            cols_to_drop.append(col_in_df)\n",
        "\n",
        "    if cols_to_drop:\n",
        "        df_out = df_out.drop(columns=list(set(cols_to_drop)))\n",
        "\n",
        "\n",
        "    # 1.\n",
        "    home_df = df_out[['match_id', 'Date', 'HomeTeam']].copy()\n",
        "    home_df.rename(columns={'HomeTeam': 'Team'}, inplace=True)\n",
        "    home_df['is_home_match'] = 1\n",
        "\n",
        "    away_df = df_out[['match_id', 'Date', 'AwayTeam']].copy()\n",
        "    away_df.rename(columns={'AwayTeam': 'Team'}, inplace=True)\n",
        "    away_df['is_home_match'] = 0\n",
        "\n",
        "    # 2.\n",
        "    all_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
        "    all_stats_df = all_stats_df.sort_values(by=['Team', 'Date'])\n",
        "\n",
        "    # 3.\n",
        "    all_stats_df['Prev_Date'] = all_stats_df.groupby('Team')['Date'].shift(1)\n",
        "\n",
        "    all_stats_df['Date'] = pd.to_datetime(all_stats_df['Date'], errors='coerce')\n",
        "    all_stats_df['Prev_Date'] = pd.to_datetime(all_stats_df['Prev_Date'], errors='coerce')\n",
        "\n",
        "    all_stats_df['TBG'] = (all_stats_df['Date'] - all_stats_df['Prev_Date']).dt.days.fillna(110)\n",
        "\n",
        "    # 4.\n",
        "    rolling_avg_series = all_stats_df.groupby('Team')['TBG'] \\\n",
        "                                     .rolling(window=window_size) \\\n",
        "                                     .mean()\n",
        "    rolling_avg_series = rolling_avg_series.reset_index(level=0, drop=True)\n",
        "\n",
        "\n",
        "    all_stats_df['TEMP_ATBG'] = rolling_avg_series\n",
        "\n",
        "\n",
        "    all_stats_df['ATBG'] = all_stats_df.groupby('Team')['TEMP_ATBG'].shift(1)\n",
        "\n",
        "    # 5.\n",
        "    all_stats_df = all_stats_df.sort_values(by='Date')\n",
        "\n",
        "    # 6.\n",
        "\n",
        "    home_prev_data = all_stats_df[all_stats_df['is_home_match'] == 1].copy()\n",
        "    home_prev_data.rename(columns={'Team': 'HomeTeam', 'ATBG': 'ATBGH'}, inplace=True)\n",
        "\n",
        "    cols_to_merge_home = ['match_id', 'HomeTeam', 'ATBGH']\n",
        "    df_out = df_out.merge(\n",
        "        home_prev_data[cols_to_merge_home],\n",
        "        on=['match_id', 'HomeTeam'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    away_prev_data = all_stats_df[all_stats_df['is_home_match'] == 0].copy()\n",
        "    away_prev_data.rename(columns={'Team': 'AwayTeam', 'ATBG': 'ATBGA'}, inplace=True)\n",
        "\n",
        "    cols_to_merge_away = ['match_id', 'AwayTeam', 'ATBGA']\n",
        "    df_out = df_out.merge(\n",
        "        away_prev_data[cols_to_merge_away],\n",
        "        on=['match_id', 'AwayTeam'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    df_out = df_out.drop(columns=['TBGH_x', 'TBGA_x', 'TBGH_y', 'TBGA_y'], errors='ignore')\n",
        "\n",
        "    return df_out"
      ],
      "metadata": {
        "id": "4HIVXytifmop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline to create the dataFrame"
      ],
      "metadata": {
        "id": "-C3zthndf_v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering_pipeline(input_df, window_size=3):\n",
        "    \"\"\"\n",
        "    Executes all feature engineering steps sequentially.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Starting pipeline with {input_df.shape[0]} matches...\")\n",
        "    df = input_df.copy()\n",
        "\n",
        "    # --- 1. Define Stat-Mapping ---\n",
        "    # (Home Stat : Away Stat)\n",
        "    avg_stats_map = {\n",
        "        'FTHG': 'FTAG',\n",
        "        'HS': 'AS',\n",
        "        'HST': 'AST',\n",
        "        'HC': 'AC',\n",
        "        'HF': 'AF',\n",
        "        'HY': 'AY',\n",
        "        'HR': 'AR'\n",
        "    }\n",
        "\n",
        "    # --- 2. Call functions sequentially ---\n",
        "\n",
        "    print(\"Classifying kickoff time (TMKO)...\")\n",
        "    df = classify_kickoff_time(df)\n",
        "\n",
        "    print(\"Calculating rolling average...\")\n",
        "    df = get_strict_rolling_avg(df, avg_stats_map, window_size=window_size)\n",
        "\n",
        "    print(\"Calculating form (last match)...\")\n",
        "    df = get_previous_game_form(df)\n",
        "\n",
        "    print(\"Calculating last match stats...\")\n",
        "    df = get_last_match_overall(df, avg_stats_map)\n",
        "\n",
        "    print(\"Calculating time between games (TBGH, TBGA)...\")\n",
        "    df = get_time_between_games(df)\n",
        "\n",
        "    print(\"Calculating number of wins in the last 3 games (NOWH, NOWA)\")\n",
        "    df = get_number_of_wins(df, window_size=window_size)\n",
        "\n",
        "    print(\"claculation avarage time between games\")\n",
        "    df = get_avg_time_between_games(df, window_size=window_size)\n",
        "\n",
        "    print(\"Pipeline finished.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "gjyIY9sugSiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. EXECUTE PIPELINE ---\n",
        "\n",
        "# Apply the entire pipeline to the prepared DataFrame - original\n",
        "features_df = feature_engineering_pipeline(df_work, window_size=3)\n",
        "\n",
        "# --- 5. CHECK RESULT ---\n",
        "\n",
        "print(\"\\n--- Result DataFrame Info ---\")\n",
        "features_df.info()\n",
        "\n",
        "# Example columns to verify the result\n",
        "print(\"\\n--- Example Data (Mid-Season) ---\")\n",
        "example_cols = [\n",
        "    'Time', 'TMKO',       #From classify_kickoff_time\n",
        "    'HomeTeam', 'AwayTeam',\n",
        "    'HST', 'AST',         # Original\n",
        "    'AHST', 'AAST',      # From get_strict_rolling_avg\n",
        "    'PHFR_Won', 'PAFR_Won', # From get_previous_game_form\n",
        "    'PHST', 'PAST'        # From get_last_match_overall\n",
        "]\n",
        "\n",
        "# Show games from mid-season\n",
        "print(features_df[example_cols].iloc[100:105])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk9Y_6j8gUNT",
        "outputId": "35bcc81c-20d7-4bee-c50c-35b59bdbb343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline with 1140 matches...\n",
            "Classifying kickoff time (TMKO)...\n",
            "Calculating rolling average...\n",
            "Calculating form (last match)...\n",
            "Calculating last match stats...\n",
            "Calculating time between games (TBGH, TBGA)...\n",
            "Calculating number of wins in the last 3 games (NOWH, NOWA)\n",
            "claculation avarage time between games\n",
            "Pipeline finished.\n",
            "\n",
            "--- Result DataFrame Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1140 entries, 0 to 1139\n",
            "Data columns (total 72 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   Date                   1140 non-null   object \n",
            " 1   Time                   1140 non-null   object \n",
            " 2   HomeTeam               1140 non-null   object \n",
            " 3   AwayTeam               1140 non-null   object \n",
            " 4   FTR                    1140 non-null   object \n",
            " 5   FTHG                   1140 non-null   int64  \n",
            " 6   FTAG                   1140 non-null   int64  \n",
            " 7   HS                     1140 non-null   int64  \n",
            " 8   AS                     1140 non-null   int64  \n",
            " 9   HST                    1140 non-null   int64  \n",
            " 10  AST                    1140 non-null   int64  \n",
            " 11  HC                     1140 non-null   int64  \n",
            " 12  AC                     1140 non-null   int64  \n",
            " 13  HF                     1140 non-null   int64  \n",
            " 14  AF                     1140 non-null   int64  \n",
            " 15  HY                     1140 non-null   int64  \n",
            " 16  AY                     1140 non-null   int64  \n",
            " 17  HR                     1140 non-null   int64  \n",
            " 18  AR                     1140 non-null   int64  \n",
            " 19  Start_Temp_C           1140 non-null   float64\n",
            " 20  Start_Wind_kmh         1140 non-null   float64\n",
            " 21  Start_Wind_Degree      1140 non-null   int64  \n",
            " 22  Start_Humidity         1140 non-null   int64  \n",
            " 23  Start_Precip_mm        1140 non-null   float64\n",
            " 24  match_id               1140 non-null   int64  \n",
            " 25  Weather_clear_sky      1140 non-null   int64  \n",
            " 26  Weather_cloudy         1140 non-null   int64  \n",
            " 27  Weather_fog            1140 non-null   int64  \n",
            " 28  Weather_heavy_rain     1140 non-null   int64  \n",
            " 29  Weather_moderate_rain  1140 non-null   int64  \n",
            " 30  Weather_rain           1140 non-null   int64  \n",
            " 31  Weather_snow           1140 non-null   int64  \n",
            " 32  Weather_thunderstorm   1140 non-null   int64  \n",
            " 33  TMKO                   1140 non-null   int64  \n",
            " 34  AFTHG                  1104 non-null   float64\n",
            " 35  AHS                    1104 non-null   float64\n",
            " 36  AHST                   1104 non-null   float64\n",
            " 37  AHC                    1104 non-null   float64\n",
            " 38  AHF                    1104 non-null   float64\n",
            " 39  AHY                    1104 non-null   float64\n",
            " 40  AHR                    1104 non-null   float64\n",
            " 41  AFTAG                  1104 non-null   float64\n",
            " 42  AAS                    1104 non-null   float64\n",
            " 43  AAST                   1104 non-null   float64\n",
            " 44  AAC                    1104 non-null   float64\n",
            " 45  AAF                    1104 non-null   float64\n",
            " 46  AAY                    1104 non-null   float64\n",
            " 47  AAR                    1104 non-null   float64\n",
            " 48  PHFR_Won               1127 non-null   float64\n",
            " 49  PHFR_NotWin            1127 non-null   float64\n",
            " 50  PAFR_Won               1129 non-null   float64\n",
            " 51  PAFR_NotWin            1129 non-null   float64\n",
            " 52  PFTHG                  1127 non-null   float64\n",
            " 53  PHS                    1127 non-null   float64\n",
            " 54  PHST                   1127 non-null   float64\n",
            " 55  PHC                    1127 non-null   float64\n",
            " 56  PHF                    1127 non-null   float64\n",
            " 57  PHY                    1127 non-null   float64\n",
            " 58  PHR                    1127 non-null   float64\n",
            " 59  PFTAG                  1129 non-null   float64\n",
            " 60  PAS                    1129 non-null   float64\n",
            " 61  PAST                   1129 non-null   float64\n",
            " 62  PAC                    1129 non-null   float64\n",
            " 63  PAF                    1129 non-null   float64\n",
            " 64  PAY                    1129 non-null   float64\n",
            " 65  PAR                    1129 non-null   float64\n",
            " 66  TBGH                   1127 non-null   float64\n",
            " 67  TBGA                   1129 non-null   float64\n",
            " 68  NOWH                   1104 non-null   float64\n",
            " 69  NOWA                   1104 non-null   float64\n",
            " 70  ATBGH                  1104 non-null   float64\n",
            " 71  ATBGA                  1104 non-null   float64\n",
            "dtypes: float64(41), int64(26), object(5)\n",
            "memory usage: 641.4+ KB\n",
            "\n",
            "--- Example Data (Mid-Season) ---\n",
            "      Time  TMKO     HomeTeam    AwayTeam  HST  AST      AHST      AAST  \\\n",
            "100  15:00     0     West Ham   Newcastle    6    9  4.000000  1.333333   \n",
            "101  12:30     0  Bournemouth  Man United    6    4  3.000000  5.333333   \n",
            "102  15:00     0      Arsenal      Wolves    4    8  3.666667  2.666667   \n",
            "103  15:00     0  Aston Villa   Liverpool    2    6  8.333333  8.333333   \n",
            "104  15:00     0     Brighton     Norwich    5    0  5.000000  3.000000   \n",
            "\n",
            "     PHFR_Won  PAFR_Won  PHST  PAST  \n",
            "100       0.0       0.0   4.0   2.0  \n",
            "101       0.0       1.0   5.0  11.0  \n",
            "102       0.0       0.0   6.0   5.0  \n",
            "103       0.0       1.0   5.0  13.0  \n",
            "104       1.0       0.0   4.0   3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.keys()"
      ],
      "metadata": {
        "id": "DHKh94AHhmPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c02b640-c59b-4477-ddf4-67ddf8c0c443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTR', 'FTHG', 'FTAG', 'HS',\n",
              "       'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR',\n",
              "       'Start_Temp_C', 'Start_Wind_kmh', 'Start_Wind_Degree', 'Start_Humidity',\n",
              "       'Start_Precip_mm', 'match_id', 'Weather_clear_sky', 'Weather_cloudy',\n",
              "       'Weather_fog', 'Weather_heavy_rain', 'Weather_moderate_rain',\n",
              "       'Weather_rain', 'Weather_snow', 'Weather_thunderstorm', 'TMKO', 'AFTHG',\n",
              "       'AHS', 'AHST', 'AHC', 'AHF', 'AHY', 'AHR', 'AFTAG', 'AAS', 'AAST',\n",
              "       'AAC', 'AAF', 'AAY', 'AAR', 'PHFR_Won', 'PHFR_NotWin', 'PAFR_Won',\n",
              "       'PAFR_NotWin', 'PFTHG', 'PHS', 'PHST', 'PHC', 'PHF', 'PHY', 'PHR',\n",
              "       'PFTAG', 'PAS', 'PAST', 'PAC', 'PAF', 'PAY', 'PAR', 'TBGH', 'TBGA',\n",
              "       'NOWH', 'NOWA', 'ATBGH', 'ATBGA'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using features:\n",
        "'Start_Temp_C', 'Start_Wind_kmh', 'Start_Wind_Degree', 'Start_Humidity',\n",
        "       'Start_Precip_mm', 'match_id', 'Weather_clear_sky', 'Weather_cloudy',\n",
        "       'Weather_fog', 'Weather_heavy_rain', 'Weather_moderate_rain',\n",
        "       'Weather_rain', 'Weather_snow', 'Weather_thunderstorm',\n",
        "            'TMKO', 'PHFR_Won', 'PAFR_Won', 'PFTHG', 'PFTAG', 'PHS', 'PAS', 'PHST', 'PAST',\n",
        "            'PHC', 'PAC', 'PHF', 'PAF', 'PHY', 'PAY', 'PHR', 'PAR', 'TBGH', 'TBGA', 'ATBGH', 'ATBGA',\n",
        "            'AFTHG', 'AFTAG', 'AHS', 'AAS', 'AHST', 'AAST', 'AHC', 'AAC', 'AHF', 'AAF', 'AHY', 'AAY',\n",
        "            'AHR', 'AAR', 'NOWH', 'NOWA'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Without weather:\n",
        "\n",
        "'TMKO', 'PHFR_Won', 'PAFR_Won', 'PFTHG', 'PFTAG', 'PHS', 'PAS', 'PHST', 'PAST',\n",
        "            'PHC', 'PAC', 'PHF', 'PAF', 'PHY', 'PAY', 'PHR', 'PAR', 'TBGH', 'TBGA', 'ATBGH', 'ATBGA',\n",
        "            'AFTHG', 'AFTAG', 'AHS', 'AAS', 'AHST', 'AAST', 'AHC', 'AAC', 'AHF', 'AAF', 'AHY', 'AAY',\n",
        "            'AHR', 'AAR', 'NOWH', 'NOWA'"
      ],
      "metadata": {
        "id": "46DZK8PWmCdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "       'Start_Temp_C', 'Start_Wind_kmh', 'Start_Wind_Degree', 'Start_Humidity',\n",
        "       'Start_Precip_mm', 'Weather_clear_sky', 'Weather_cloudy',\n",
        "       'Weather_fog', 'Weather_heavy_rain', 'Weather_moderate_rain',\n",
        "       'Weather_rain', 'Weather_snow', 'Weather_thunderstorm',\n",
        "            'TMKO', 'PHFR_Won', 'PAFR_Won', 'PFTHG', 'PFTAG', 'PHS', 'PAS', 'PHST', 'PAST',\n",
        "            'PHC', 'PAC', 'PHF', 'PAF', 'PHY', 'PAY', 'PHR', 'PAR', 'TBGH', 'TBGA', 'ATBGH', 'ATBGA',\n",
        "            'AFTHG', 'AFTAG', 'AHS', 'AAS', 'AHST', 'AAST', 'AHC', 'AAC', 'AHF', 'AAF', 'AHY', 'AAY',\n",
        "            'AHR', 'AAR', 'NOWH', 'NOWA']"
      ],
      "metadata": {
        "id": "y461qpuqcCDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training, Testing"
      ],
      "metadata": {
        "id": "EcsMadVPaYei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clean up to length\n",
        "df_clean = features_df.dropna().copy()\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. check format\n",
        "df_clean['Date'] = pd.to_datetime(df_clean['Date'])\n",
        "\n",
        "# 2. Create a target variable (1 = home win, 0 = no home win)\n",
        "df_clean['HomeWin_Binary'] = df_clean['FTR'].apply(lambda x: 1 if x == 'H' else 0)\n",
        "\n",
        "# 3. Chronological split (time separation)\n",
        "split_date = '2021-08-12'\n",
        "\n",
        "# Everything before or on August 13, 2021 is training.\n",
        "train_df = df_clean[df_clean['Date'] <= split_date].copy()\n",
        "\n",
        "# Everything after is testing\n",
        "test_df = df_clean[df_clean['Date'] > split_date].copy()\n",
        "\n",
        "print(f\"Training till {split_date}: {len(train_df)} games\")\n",
        "print(f\"Testing since {split_date}:       {len(test_df)} games\")\n",
        "\n",
        "# 4. Assign features and targets\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['HomeWin_Binary']\n",
        "\n",
        "X_test = test_df[features].dropna()\n",
        "# dropping for consistency with paper\n",
        "X_test = X_test.iloc[27:]\n",
        "y_test = test_df['HomeWin_Binary']\n",
        "y_test = y_test.iloc[27:]\n",
        "\n",
        "X_train.to_csv(\"X_train.csv\",index=False)\n",
        "df_clean.to_csv(\"df_clean.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3SsZowwfINd",
        "outputId": "362310c9-8d10-429c-d406-da0e355a12b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training till 2021-08-12: 722 games\n",
            "Testing since 2021-08-12:       377 games\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZUYhme3QmWC",
        "outputId": "cfb42d28-5345-4ac2-97cb-30ae60440e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((722, 50), (722,), (350, 50), (350,))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the models"
      ],
      "metadata": {
        "id": "HPL7w1ZFjOEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import svm\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "X2BXPtYOmtZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name=\"Modell\"):\n",
        "    # 1. Training\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 2. Prediction\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # 3. Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "    # 4.\n",
        "    print(f\"\\n{'='*10} Results: {model_name} {'='*10}\")\n",
        "    print(f\"TP (True Home victory): {tp}\")\n",
        "    print(f\"TN (Home loss detected): {tn}\")\n",
        "    print(f\"FP (False alarm):          {fp}\")\n",
        "    print(f\"FN (Missed win):           {fn}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f} \")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"MCC:       {mcc:.4f}\")\n",
        "    print(\"\\n=== Classification Report ===\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xsulRAxqjGgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_lr = {'C': 1, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.0001}\n",
        "param_rf = {'max_depth': 5, 'max_features': 0.8, 'max_samples': 1.0, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 50}\n",
        "param_svm = {'C': 1, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf', 'tol': 0.001}\n",
        "param_xgb = {'colsample_bytree': 1.0, 'lambda': 10, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
        "param_gbm = {'max_depth': 4, 'num_leaves': 42, 'learning_rate': 0.13867651282378762, 'min_child_samples': 57, 'subsample': 0.6231794015571818, 'colsample_bytree': 0.6335058814681457, 'lambda_l1': 8.071956365090848, 'lambda_l2': 2.8873424651371224}\n"
      ],
      "metadata": {
        "id": "LMwUFoVuBTO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition der Modelle\n",
        "models = {\n",
        "    \"Logistic Regression\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('lr', LogisticRegression(C=1.0, solver='liblinear', penalty=\"l2\", tol=0.0001))\n",
        "    ]),\n",
        "\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        max_depth=5,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=5,\n",
        "        n_estimators= 50,\n",
        "        max_features=0.8,\n",
        "        max_samples= 1.0,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    \"SVM\": Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', svm.SVC(C=1, class_weight= \"balanced\", kernel='rbf', gamma= 0.01, probability=True, tol=0.001))\n",
        "    ]),\n",
        "\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=150,\n",
        "        learning_rate=0.01,\n",
        "        max_depth=5,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree = 1.0,\n",
        "        reg_lambda = 10,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    \"Gradient Boosting (GBM)\": GradientBoostingClassifier(\n",
        "        max_depth=4,\n",
        "        learning_rate= 0.1387,\n",
        "        subsample=0.623,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    \"LightGBM\": LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1387,\n",
        "        num_leaves=42,\n",
        "        min_child_samples=57,\n",
        "        subsample=0.623,\n",
        "        colsample_bytree=0.634,\n",
        "        reg_lambda=2.89,\n",
        "        max_depth=-4,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Loop over all models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Trainiere {name}...\")\n",
        "    trained_model = train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name=name)\n",
        "    results[name] = trained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y6IeuKLuhSSy",
        "outputId": "8c7b18f1-2231-45fa-a3f1-427b86ff0f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainiere Logistic Regression...\n",
            "\n",
            "========== Results: Logistic Regression ==========\n",
            "TP (True Home victory): 60\n",
            "TN (Home loss detected): 155\n",
            "FP (False alarm):          46\n",
            "FN (Missed win):           89\n",
            "------------------------------\n",
            "Accuracy:  0.6143\n",
            "Precision: 0.5660 \n",
            "Recall:    0.4027\n",
            "F1-Score:  0.4706\n",
            "MCC:       0.1871\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.77      0.70       201\n",
            "           1       0.57      0.40      0.47       149\n",
            "\n",
            "    accuracy                           0.61       350\n",
            "   macro avg       0.60      0.59      0.58       350\n",
            "weighted avg       0.61      0.61      0.60       350\n",
            "\n",
            "Trainiere Random Forest...\n",
            "\n",
            "========== Results: Random Forest ==========\n",
            "TP (True Home victory): 51\n",
            "TN (Home loss detected): 161\n",
            "FP (False alarm):          40\n",
            "FN (Missed win):           98\n",
            "------------------------------\n",
            "Accuracy:  0.6057\n",
            "Precision: 0.5604 \n",
            "Recall:    0.3423\n",
            "F1-Score:  0.4250\n",
            "MCC:       0.1615\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.80      0.70       201\n",
            "           1       0.56      0.34      0.42       149\n",
            "\n",
            "    accuracy                           0.61       350\n",
            "   macro avg       0.59      0.57      0.56       350\n",
            "weighted avg       0.60      0.61      0.58       350\n",
            "\n",
            "Trainiere SVM...\n",
            "\n",
            "========== Results: SVM ==========\n",
            "TP (True Home victory): 80\n",
            "TN (Home loss detected): 136\n",
            "FP (False alarm):          65\n",
            "FN (Missed win):           69\n",
            "------------------------------\n",
            "Accuracy:  0.6171\n",
            "Precision: 0.5517 \n",
            "Recall:    0.5369\n",
            "F1-Score:  0.5442\n",
            "MCC:       0.2143\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.68      0.67       201\n",
            "           1       0.55      0.54      0.54       149\n",
            "\n",
            "    accuracy                           0.62       350\n",
            "   macro avg       0.61      0.61      0.61       350\n",
            "weighted avg       0.62      0.62      0.62       350\n",
            "\n",
            "Trainiere XGBoost...\n",
            "\n",
            "========== Results: XGBoost ==========\n",
            "TP (True Home victory): 39\n",
            "TN (Home loss detected): 175\n",
            "FP (False alarm):          26\n",
            "FN (Missed win):           110\n",
            "------------------------------\n",
            "Accuracy:  0.6114\n",
            "Precision: 0.6000 \n",
            "Recall:    0.2617\n",
            "F1-Score:  0.3645\n",
            "MCC:       0.1683\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.87      0.72       201\n",
            "           1       0.60      0.26      0.36       149\n",
            "\n",
            "    accuracy                           0.61       350\n",
            "   macro avg       0.61      0.57      0.54       350\n",
            "weighted avg       0.61      0.61      0.57       350\n",
            "\n",
            "Trainiere Gradient Boosting (GBM)...\n",
            "\n",
            "========== Results: Gradient Boosting (GBM) ==========\n",
            "TP (True Home victory): 67\n",
            "TN (Home loss detected): 141\n",
            "FP (False alarm):          60\n",
            "FN (Missed win):           82\n",
            "------------------------------\n",
            "Accuracy:  0.5943\n",
            "Precision: 0.5276 \n",
            "Recall:    0.4497\n",
            "F1-Score:  0.4855\n",
            "MCC:       0.1554\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.70      0.67       201\n",
            "           1       0.53      0.45      0.49       149\n",
            "\n",
            "    accuracy                           0.59       350\n",
            "   macro avg       0.58      0.58      0.58       350\n",
            "weighted avg       0.59      0.59      0.59       350\n",
            "\n",
            "Trainiere LightGBM...\n",
            "\n",
            "========== Results: LightGBM ==========\n",
            "TP (True Home victory): 71\n",
            "TN (Home loss detected): 150\n",
            "FP (False alarm):          51\n",
            "FN (Missed win):           78\n",
            "------------------------------\n",
            "Accuracy:  0.6314\n",
            "Precision: 0.5820 \n",
            "Recall:    0.4765\n",
            "F1-Score:  0.5240\n",
            "MCC:       0.2312\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.75      0.70       201\n",
            "           1       0.58      0.48      0.52       149\n",
            "\n",
            "    accuracy                           0.63       350\n",
            "   macro avg       0.62      0.61      0.61       350\n",
            "weighted avg       0.63      0.63      0.62       350\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble"
      ],
      "metadata": {
        "id": "t7ZlHor5TV3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group 1\n",
        "estimators_all = [\n",
        "    ('lr', models['Logistic Regression']),\n",
        "    ('rf', models['Random Forest']),\n",
        "    ('svm', models['SVM']),\n",
        "    ('xgb', models['XGBoost']),\n",
        "    ('gbm', models['Gradient Boosting (GBM)'])\n",
        "]\n",
        "\n",
        "# Group 2\n",
        "estimators_subset = [\n",
        "    ('lr', models['Logistic Regression']),\n",
        "    ('svm', models['SVM']),\n",
        "    ('lgbm', models['LightGBM'],)\n",
        "]"
      ],
      "metadata": {
        "id": "A0Fv-1cCTYei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking\n",
        "stack_all = StackingClassifier(\n",
        "    estimators=estimators_all,\n",
        "    final_estimator= LogisticRegression(),\n",
        "    #final_estimator = svm.SVC(C=1, class_weight= \"balanced\", kernel='rbf', gamma= 0.01, probability=True),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Voting\n",
        "vote_all = VotingClassifier(\n",
        "    estimators=estimators_all,\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "\n",
        "# --- Szenario 2: Subset (LR, SVM, LightGBM) ---\n",
        "\n",
        "# Stacking\n",
        "stack_subset = StackingClassifier(\n",
        "    estimators=estimators_subset,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    #final_estimator = svm.SVC(C=1, class_weight= \"balanced\", kernel='rbf', gamma= 0.01, probability=True),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Voting\n",
        "vote_subset = VotingClassifier(\n",
        "    estimators=estimators_subset,\n",
        "    voting='hard'\n",
        ")\n"
      ],
      "metadata": {
        "id": "M6IigcP-U0pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "choosing the final estimator makes a big difference for the stacking. With svm only home wins are picked"
      ],
      "metadata": {
        "id": "3liDfGmShoZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_models = {\n",
        "    \"Stacking (All)\": stack_all,\n",
        "    \"Voting (All)\": vote_all,\n",
        "    \"Stacking (Subset)\": stack_subset,\n",
        "    \"Voting (Subset)\": vote_subset\n",
        "}\n",
        "\n",
        "models.update(ensemble_models)"
      ],
      "metadata": {
        "id": "5A9K5RKrVl9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "bZI-xEciUflX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "# 1. Empty list for results\n",
        "results = []\n",
        "\n",
        "print(\"Start training of models...\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Training\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prediction\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate matrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, pos_label=1)\n",
        "    rec = recall_score(y_test, y_pred, pos_label=1)\n",
        "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "    # Save results\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1-Score\": f1,\n",
        "        \"MCC\": mcc\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "df_results = df_results.round(4)\n",
        "\n",
        "transposed = df_results.transpose()\n",
        "transposed.to_csv(\"results_paper_with_weather_lin_estimator.csv\", index=True)\n",
        "\n",
        "# show table\n",
        "display(transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "-3OMSrBotTEo",
        "outputId": "ea5e25e7-ac10-4be3-c003-96c2e0600fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training of models...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                             0              1       2        3  \\\n",
              "Model      Logistic Regression  Random Forest     SVM  XGBoost   \n",
              "Accuracy                0.6143         0.6057  0.6171   0.6114   \n",
              "Precision                0.566         0.5604  0.5517      0.6   \n",
              "Recall                  0.4027         0.3423  0.5369   0.2617   \n",
              "F1-Score                0.4706          0.425  0.5442   0.3645   \n",
              "MCC                     0.1871         0.1615  0.2143   0.1683   \n",
              "\n",
              "                                 4         5               6             7  \\\n",
              "Model      Gradient Boosting (GBM)  LightGBM  Stacking (All)  Voting (All)   \n",
              "Accuracy                    0.5943    0.6314          0.5743        0.6086   \n",
              "Precision                   0.5276     0.582             0.5        0.5682   \n",
              "Recall                      0.4497    0.4765          0.0067        0.3356   \n",
              "F1-Score                    0.4855     0.524          0.0132        0.4219   \n",
              "MCC                         0.1554    0.2312          0.0114         0.167   \n",
              "\n",
              "                           8                9  \n",
              "Model      Stacking (Subset)  Voting (Subset)  \n",
              "Accuracy              0.5714           0.6343  \n",
              "Precision                0.4           0.5827  \n",
              "Recall                0.0134           0.4966  \n",
              "F1-Score               0.026           0.5362  \n",
              "MCC                  -0.0063           0.2396  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b89b77c-9f0d-4377-bcf4-27007201d8e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>Random Forest</td>\n",
              "      <td>SVM</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>Gradient Boosting (GBM)</td>\n",
              "      <td>LightGBM</td>\n",
              "      <td>Stacking (All)</td>\n",
              "      <td>Voting (All)</td>\n",
              "      <td>Stacking (Subset)</td>\n",
              "      <td>Voting (Subset)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.6143</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>0.6171</td>\n",
              "      <td>0.6114</td>\n",
              "      <td>0.5943</td>\n",
              "      <td>0.6314</td>\n",
              "      <td>0.5743</td>\n",
              "      <td>0.6086</td>\n",
              "      <td>0.5714</td>\n",
              "      <td>0.6343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Precision</th>\n",
              "      <td>0.566</td>\n",
              "      <td>0.5604</td>\n",
              "      <td>0.5517</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.5276</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5682</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recall</th>\n",
              "      <td>0.4027</td>\n",
              "      <td>0.3423</td>\n",
              "      <td>0.5369</td>\n",
              "      <td>0.2617</td>\n",
              "      <td>0.4497</td>\n",
              "      <td>0.4765</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.3356</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0.4966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F1-Score</th>\n",
              "      <td>0.4706</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.5442</td>\n",
              "      <td>0.3645</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.0132</td>\n",
              "      <td>0.4219</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.5362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MCC</th>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.1615</td>\n",
              "      <td>0.2143</td>\n",
              "      <td>0.1683</td>\n",
              "      <td>0.1554</td>\n",
              "      <td>0.2312</td>\n",
              "      <td>0.0114</td>\n",
              "      <td>0.167</td>\n",
              "      <td>-0.0063</td>\n",
              "      <td>0.2396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b89b77c-9f0d-4377-bcf4-27007201d8e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b89b77c-9f0d-4377-bcf4-27007201d8e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b89b77c-9f0d-4377-bcf4-27007201d8e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-68fa801d-af9d-424d-be29-6498892d73e8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68fa801d-af9d-424d-be29-6498892d73e8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-68fa801d-af9d-424d-be29-6498892d73e8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d0f71b79-942e-4fab-991b-3dbc31ad4108\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('transposed')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d0f71b79-942e-4fab-991b-3dbc31ad4108 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('transposed');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "transposed",
              "summary": "{\n  \"name\": \"transposed\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Logistic Regression\",\n          0.6143,\n          0.1871\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Random Forest\",\n          0.6057,\n          0.1615\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"SVM\",\n          0.6171,\n          0.2143\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"XGBoost\",\n          0.6114,\n          0.1683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Gradient Boosting (GBM)\",\n          0.5943,\n          0.1554\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"LightGBM\",\n          0.6314,\n          0.2312\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Stacking (All)\",\n          0.5743,\n          0.0114\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Voting (All)\",\n          0.6086,\n          0.167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Stacking (Subset)\",\n          0.5714,\n          -0.0063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Voting (Subset)\",\n          0.6343,\n          0.2396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}